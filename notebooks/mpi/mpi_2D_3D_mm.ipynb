{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D and 3D Matrix Multiplication\n",
    "## Setup\n",
    "### Installation\n",
    "```pip install ipyparallel```\n",
    "\n",
    "or \n",
    "\n",
    "```pip install -e .[notebook]```\n",
    "\n",
    "### Start cluster\n",
    "\n",
    "```ipcluster start -n 4 --engines=MPI --profile mpi```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile='mpi')\n",
    "rc.wait_for_engines(4)\n",
    "len(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] Hello from rank 1!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] Hello from rank 0!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] Hello from rank 2!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] Hello from rank 3!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import torch\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print(f'Hello from rank {rank}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] Hello from rank 1! My coordinates are [0, 1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] Hello from rank 3! My coordinates are [1, 1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] Hello from rank 0! My coordinates are [0, 0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] Hello from rank 2! My coordinates are [1, 0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "cart_comm = comm.Create_cart(dims=[2, 2], periods=[True, True], reorder=True)\n",
    "print(f'Hello from rank {rank}! My coordinates are {cart_comm.Get_coords(rank)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] Topo: ([2, 2], [1, 1], [0, 1])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] Topo: ([2, 2], [1, 1], [0, 0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] Topo: ([2, 2], [1, 1], [1, 0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] Topo: ([2, 2], [1, 1], [1, 1])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "print(f\"Topo: {cart_comm.Get_topo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] Hello from rank 0! In my row, the ranks are [0, 1]\n",
       "Hello from rank 0! In my col, the ranks are [0, 2]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] Hello from rank 3! In my row, the ranks are [2, 3]\n",
       "Hello from rank 3! In my col, the ranks are [1, 3]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Hello from rank 1! In my row, the ranks are [0, 1]\n",
       "Hello from rank 1! In my col, the ranks are [1, 3]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] Hello from rank 2! In my row, the ranks are [2, 3]\n",
       "Hello from rank 2! In my col, the ranks are [0, 2]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "row_comm = cart_comm.Sub([0,1])\n",
    "row_global_ranks = row_comm.allgather(rank)\n",
    "print(f'Hello from rank {rank}! In my row, the ranks are {row_global_ranks}')\n",
    "\n",
    "col_comm = cart_comm.Sub([1,0])\n",
    "col_global_ranks = col_comm.allgather(rank)\n",
    "print(f'Hello from rank {rank}! In my col, the ranks are {col_global_ranks}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def as_buffer(x: torch.Tensor):\n",
    "    return MPI.buffer.fromaddress(x.untyped_storage().data_ptr(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Vector\n",
    "\n",
    "### y:= Ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] A_local: tensor([[ 0,  2,  4,  6],\n",
       "        [16, 18, 20, 22],\n",
       "        [32, 34, 36, 38],\n",
       "        [48, 50, 52, 54]])\n",
       "x_local: tensor([[0],\n",
       "        [4]])\n",
       "x_gather_col: tensor([[0],\n",
       "        [2],\n",
       "        [4],\n",
       "        [6]])\n",
       "y_local: tensor([[ 56],\n",
       "        [248],\n",
       "        [440],\n",
       "        [632]])\n",
       "y_scatter: tensor([[ 140],\n",
       "        [1036]])\n",
       "y_end: tensor([[ 140],\n",
       "        [ 364],\n",
       "        [ 588],\n",
       "        [ 812],\n",
       "        [1036],\n",
       "        [1260],\n",
       "        [1484],\n",
       "        [1708]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] A_local: tensor([[ 9, 11, 13, 15],\n",
       "        [25, 27, 29, 31],\n",
       "        [41, 43, 45, 47],\n",
       "        [57, 59, 61, 63]])\n",
       "x_local: tensor([[3],\n",
       "        [7]])\n",
       "x_gather_col: tensor([[1],\n",
       "        [3],\n",
       "        [5],\n",
       "        [7]])\n",
       "y_local: tensor([[212],\n",
       "        [468],\n",
       "        [724],\n",
       "        [980]])\n",
       "y_scatter: tensor([[ 812],\n",
       "        [1708]])\n",
       "y_end: tensor([[ 140],\n",
       "        [ 364],\n",
       "        [ 588],\n",
       "        [ 812],\n",
       "        [1036],\n",
       "        [1260],\n",
       "        [1484],\n",
       "        [1708]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] A_local: tensor([[ 8, 10, 12, 14],\n",
       "        [24, 26, 28, 30],\n",
       "        [40, 42, 44, 46],\n",
       "        [56, 58, 60, 62]])\n",
       "x_local: tensor([[2],\n",
       "        [6]])\n",
       "x_gather_col: tensor([[0],\n",
       "        [2],\n",
       "        [4],\n",
       "        [6]])\n",
       "y_local: tensor([[152],\n",
       "        [344],\n",
       "        [536],\n",
       "        [728]])\n",
       "y_scatter: tensor([[ 364],\n",
       "        [1260]])\n",
       "y_end: tensor([[ 140],\n",
       "        [ 364],\n",
       "        [ 588],\n",
       "        [ 812],\n",
       "        [1036],\n",
       "        [1260],\n",
       "        [1484],\n",
       "        [1708]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] A_local: tensor([[ 1,  3,  5,  7],\n",
       "        [17, 19, 21, 23],\n",
       "        [33, 35, 37, 39],\n",
       "        [49, 51, 53, 55]])\n",
       "x_local: tensor([[1],\n",
       "        [5]])\n",
       "x_gather_col: tensor([[1],\n",
       "        [3],\n",
       "        [5],\n",
       "        [7]])\n",
       "y_local: tensor([[ 84],\n",
       "        [340],\n",
       "        [596],\n",
       "        [852]])\n",
       "y_scatter: tensor([[ 588],\n",
       "        [1484]])\n",
       "y_end: tensor([[ 140],\n",
       "        [ 364],\n",
       "        [ 588],\n",
       "        [ 812],\n",
       "        [1036],\n",
       "        [1260],\n",
       "        [1484],\n",
       "        [1708]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "A_g = torch.arange(64).reshape(8, 8)\n",
    "A_l = A_g[col_comm.Get_rank()::2, row_comm.Get_rank()::2].contiguous()\n",
    "print(\"A_local:\", A_l)\n",
    "\n",
    "x_g = torch.arange(8).reshape(8, 1)\n",
    "x_l = x_g[comm.Get_rank()::comm.Get_size(), :].contiguous()\n",
    "print(\"x_local:\", x_l)\n",
    "\n",
    "\n",
    "# x_col = torch.zeros(4,1, dtype=torch.long)\n",
    "# col_comm.Allgather((as_buffer(x_l), 2, MPI.LONG), (as_buffer(x_col), 2, MPI.LONG))\n",
    "# x_col = x_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "# print(\"x_gather_col:\", x_col)\n",
    "\n",
    "x_col = torch.zeros(4,1, dtype=torch.long)\n",
    "data_type = MPI.LONG.Create_vector(2, 1, 2).Create_resized(MPI.LONG.Get_extent()[0], MPI.LONG.Get_extent()[1]).Commit()\n",
    "col_comm.Allgather((as_buffer(x_l), 2, MPI.LONG), (as_buffer(x_col), 1, data_type))\n",
    "print(\"x_gather_col:\", x_col)\n",
    "\n",
    "y_l = A_l @ x_col\n",
    "print(\"y_local:\", y_l)\n",
    "\n",
    "y_l = y_l.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "y_scatter = torch.zeros(2, 1, dtype=torch.long)\n",
    "row_comm.Reduce_scatter((as_buffer(y_l), 4, MPI.LONG), (as_buffer(y_scatter), 2, MPI.LONG), [2,2], MPI.SUM)\n",
    "print(\"y_scatter:\", y_scatter)\n",
    "\n",
    "y_end = torch.zeros(2,2,2, dtype=torch.long)\n",
    "comm.Allgather((as_buffer(y_scatter), 2, MPI.LONG), (as_buffer(y_end), 2, MPI.LONG))\n",
    "y_end = y_end.permute(2,1,0).reshape(8,1).contiguous()\n",
    "print(\"y_end:\", y_end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"Expected:\", A_g @ x_g)\n",
    "print(\"Actual:\", y_end)\n",
    "torch.allclose(y_end, A_g @ x_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x = A.T * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A_g = torch.arange(64).reshape(8, 8)\n",
    "A_l = A_g[col_comm.Get_rank()::2, row_comm.Get_rank()::2].contiguous()\n",
    "print(\"A_local:\", A_l)\n",
    "\n",
    "y_g = torch.arange(8).reshape(8, 1)\n",
    "i = col_comm.Get_rank() + col_comm.Get_size() * row_comm.Get_rank()\n",
    "y_l = y_g[i::comm.Get_size(), :].contiguous()\n",
    "print(\"y_local:\", y_l)\n",
    "\n",
    "y_col = torch.zeros(4,1, dtype=torch.long)\n",
    "row_comm.Allgather((as_buffer(y_l), 2, MPI.LONG), (as_buffer(y_col), 2, MPI.LONG))\n",
    "y_col = y_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "print(\"y_gather_col:\", y_col)\n",
    "\n",
    "x_l = A_l.T @ y_col\n",
    "print(\"x_local:\", x_l)\n",
    "\n",
    "x_l = x_l.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "x_scatter = torch.zeros(2, 1, dtype=torch.long)\n",
    "col_comm.Reduce_scatter((as_buffer(x_l), 4, MPI.LONG), (as_buffer(x_scatter), 2, MPI.LONG), [2,2], MPI.SUM)\n",
    "print(\"x_scatter:\", x_scatter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"expected:\", A_g.T @ y_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A := y * x.T + A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A_g = torch.arange(64).reshape(8, 8)\n",
    "A_l = A_g[col_comm.Get_rank()::2, row_comm.Get_rank()::2].contiguous()\n",
    "print(\"A_local:\", A_l)\n",
    "\n",
    "x_g = torch.arange(8).reshape(8, 1)\n",
    "x_l = x_g[comm.Get_rank()::comm.Get_size(), :].contiguous()\n",
    "print(\"x_local:\", x_l)\n",
    "\n",
    "y_g = torch.arange(8).reshape(8, 1)\n",
    "i = col_comm.Get_rank() + col_comm.Get_size() * row_comm.Get_rank()\n",
    "y_l = y_g[i::comm.Get_size(), :].contiguous()\n",
    "print(\"y_local:\", y_l)\n",
    "\n",
    "x_col = torch.zeros(4,1, dtype=torch.long)\n",
    "col_comm.Allgather((as_buffer(x_l), 2, MPI.LONG), (as_buffer(x_col), 2, MPI.LONG))\n",
    "x_col = x_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "print(\"x_gather_col:\", x_col)\n",
    "\n",
    "y_col = torch.zeros(4,1, dtype=torch.long)\n",
    "row_comm.Allgather((as_buffer(y_l), 2, MPI.LONG), (as_buffer(y_col), 2, MPI.LONG))\n",
    "y_col = y_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "print(\"y_gather_col:\", y_col)\n",
    "\n",
    "Z_l = y_col @ x_col.T + A_l\n",
    "print(\"Z_local:\", Z_l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(f\"Expected: {y_g @ x_g.T + A_g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create_darray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if comm.Get_rank() == 0:\n",
    "    A = torch.arange(64).reshape(8, 8)\n",
    "    print(A)\n",
    "    print(A.dtype)\n",
    "\n",
    "    darray_type = MPI.LONG.Create_darray(\n",
    "        4,                    # Size\n",
    "        1,                      # Rank\n",
    "        # 2,                      # number of array dimensions (as well as process grid dimensions)\n",
    "        [64,1],                 # size of the global array\n",
    "        [MPI.DISTRIBUTE_CYCLIC, MPI.DISTRIBUTE_NONE], # distribution type\n",
    "        [1, 1], # distribution argument\n",
    "        [4, 1],                 # size of the process grid\n",
    "        MPI.ORDER_C,            # array storage order\n",
    "    ).Commit()\n",
    "\n",
    "    # comm.Send(buf=[as_buffer(A), 8, MPI.LONG], dest=1)\n",
    "    comm.Send([as_buffer(A), 1, darray_type], dest=1, tag=55)\n",
    "\n",
    "    darray_type.Free() \n",
    "\n",
    "elif comm.Get_rank() == 1:\n",
    "    A = torch.zeros(4, 4, dtype=torch.int64)\n",
    "\n",
    "    # comm.Recv(buf=[as_buffer(A), 8, MPI.LONG], source=0)\n",
    "    comm.Recv([as_buffer(A), 16, MPI.LONG], source=0, tag=55)\n",
    "    print(A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Item Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import tensorcraft as tc\n",
    "comm = MPI.COMM_WORLD\n",
    "## Let's find the maximum element count on my mpi implementation\n",
    "options = [torch.iinfo(torch.int32).max]\n",
    "\n",
    "for possible_max in options:\n",
    "    print(f\"Trying {possible_max}\")\n",
    "    if comm.Get_rank() == 0:\n",
    "        A = torch.ones(possible_max, dtype=torch.bool)\n",
    "        print(A.dtype)\n",
    "        print(A[:10])\n",
    "\n",
    "        print(f\"Sending {possible_max} elements, {possible_max / 10**9} Gb\")\n",
    "\n",
    "    else:\n",
    "        A = torch.zeros(possible_max, dtype=torch.bool)\n",
    "\n",
    "    comm.Bcast(buf=[tc.mpi4torch.as_buffer(A), possible_max, MPI.BOOL], root=0)\n",
    "\n",
    "    if comm.Get_rank() == 0:\n",
    "        print(\"Sent!\")\n",
    "    else:\n",
    "        print(\"Received!\")\n",
    "        print(A[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interweave allgather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile='mpi')\n",
    "rc.wait_for_engines(4)\n",
    "len(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:2] Hello from rank 2!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] Hello from rank 3!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] Hello from rank 0!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Hello from rank 1!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import torch\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print(f'Hello from rank {rank}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:3] 17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Processor multi index: torch.Size([1, 1])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Missing elements: [3, 0]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: N blocks per axis: [6, 1]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Padded tensor shape: torch.Size([21, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Permute tuple: (0, 2, 1, 3)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Reshape tuple: [7, 3, 1, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Tile Slices: [slice(tensor(3), None, 4), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Local tensor shape: torch.Size([1, 1, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Target local tensor shape: [3, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Linear processor index: 3, Residue: 3, Block size: 3, axis: 0, N procs: 4, N full blocks: 6\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Shaved slices: [slice(None, None, None), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R3: Final local tensor shape: torch.Size([3, 5])\n",
       "tensor([[45, 46, 47, 48, 49],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [55, 56, 57, 58, 59]])\n",
       "torch.Size([3, 5])\n",
       "torch.int64\n",
       "True\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] 17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Processor multi index: torch.Size([1, 0])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Missing elements: [3, 0]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: N blocks per axis: [6, 1]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Padded tensor shape: torch.Size([21, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Permute tuple: (0, 2, 1, 3)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Reshape tuple: [7, 3, 1, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Tile Slices: [slice(tensor(2), None, 4), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Local tensor shape: torch.Size([2, 1, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Target local tensor shape: [6, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Linear processor index: 2, Residue: 3, Block size: 3, axis: 0, N procs: 4, N full blocks: 6\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Shaved slices: [slice(0, -3, None), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R2: Final local tensor shape: torch.Size([3, 5])\n",
       "tensor([[30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39],\n",
       "        [40, 41, 42, 43, 44]])\n",
       "torch.Size([3, 5])\n",
       "torch.int64\n",
       "True\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] 17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Processor multi index: torch.Size([0, 0])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Missing elements: [3, 0]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: N blocks per axis: [6, 1]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Padded tensor shape: torch.Size([21, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Permute tuple: (0, 2, 1, 3)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Reshape tuple: [7, 3, 1, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Tile Slices: [slice(tensor(0), None, 4), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Local tensor shape: torch.Size([2, 1, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Target local tensor shape: [6, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Linear processor index: 0, Residue: 3, Block size: 3, axis: 0, N procs: 4, N full blocks: 6\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Shaved slices: [slice(None, None, None), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R0: Final local tensor shape: torch.Size([6, 5])\n",
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [60, 61, 62, 63, 64],\n",
       "        [65, 66, 67, 68, 69],\n",
       "        [70, 71, 72, 73, 74]])\n",
       "torch.Size([6, 5])\n",
       "torch.int64\n",
       "True\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] 17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Processor multi index: torch.Size([0, 1])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Missing elements: [3, 0]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: N blocks per axis: [6, 1]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Padded tensor shape: torch.Size([21, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Permute tuple: (0, 2, 1, 3)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Reshape tuple: [7, 3, 1, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Tile Slices: [slice(tensor(1), None, 4), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Local tensor shape: torch.Size([2, 1, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Target local tensor shape: [6, 5]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Linear processor index: 1, Residue: 3, Block size: 3, axis: 0, N procs: 4, N full blocks: 6\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Shaved slices: [slice(None, None, None), slice(None, None, None)]\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply -- R1: Final local tensor shape: torch.Size([6, 5])\n",
       "tensor([[15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [75, 76, 77, 78, 79],\n",
       "        [80, 81, 82, 83, 84],\n",
       "        [85, 86, 87, 88, 89]])\n",
       "torch.Size([6, 5])\n",
       "torch.int64\n",
       "True\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import logging\n",
    "import tensorcraft as tc\n",
    "\n",
    "log = logging.getLogger('tensorcraft')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "x = torch.arange(90).reshape(18, 5)\n",
    "mesh = torch.Size([2,2])\n",
    "dist = tc.dist.MultiAxisDist(mesh, ((0,1), None), 3)\n",
    "\n",
    "x_local = dist.apply(x, rank)\n",
    "print(x_local)\n",
    "print(x_local.shape)\n",
    "print(x_local.dtype)\n",
    "print(x_local.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] 17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Local tensor shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Expected local shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Changed tensor axis: 0, minor: False\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: New distribution: D_[2,2]⊥{1,∅}(3,∅), new shape: torch.Size([9, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Processor multi index: torch.Size([0, 1])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: N procs: 2\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Rank of largest tensor in the subcommunicator: [0, 1] 1\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: N elements: 30\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Max local shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Send buffer: (<mpi4py.MPI.buffer object at 0x7218b0bd8570>, 30, <mpi4py.MPI.Datatype object at 0x7219a4548a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Recv buffer: (<mpi4py.MPI.buffer object at 0x7218b0bd8490>, 30, <mpi4py.MPI.Datatype object at 0x7219a4548a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Recv_tensor : tensor([[[[15, 16, 17, 18, 19],\n",
       "          [20, 21, 22, 23, 24],\n",
       "          [25, 26, 27, 28, 29]],\n",
       "\n",
       "         [[75, 76, 77, 78, 79],\n",
       "          [80, 81, 82, 83, 84],\n",
       "          [85, 86, 87, 88, 89]]],\n",
       "\n",
       "\n",
       "        [[[45, 46, 47, 48, 49],\n",
       "          [50, 51, 52, 53, 54],\n",
       "          [55, 56, 57, 58, 59]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Permuted data tensor shape: torch.Size([2, 2, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R1: Permuted data tensor: tensor([[[[15, 16, 17, 18, 19],\n",
       "          [20, 21, 22, 23, 24],\n",
       "          [25, 26, 27, 28, 29]],\n",
       "\n",
       "         [[45, 46, 47, 48, 49],\n",
       "          [50, 51, 52, 53, 54],\n",
       "          [55, 56, 57, 58, 59]]],\n",
       "\n",
       "\n",
       "        [[[75, 76, 77, 78, 79],\n",
       "          [80, 81, 82, 83, 84],\n",
       "          [85, 86, 87, 88, 89]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] 17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Local tensor shape: torch.Size([3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Expected local shape: torch.Size([3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Changed tensor axis: 0, minor: False\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: New distribution: D_[2,2]⊥{1,∅}(3,∅), new shape: torch.Size([9, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Processor multi index: torch.Size([1, 0])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: N procs: 2\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Rank of largest tensor in the subcommunicator: [0, 0] 0\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: N elements: 30\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Max local shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Send buffer: (<mpi4py.MPI.buffer object at 0x7e71d6501140>, 15, <mpi4py.MPI.Datatype object at 0x7e72c9f14a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Recv buffer: (<mpi4py.MPI.buffer object at 0x7e71d6501220>, 30, <mpi4py.MPI.Datatype object at 0x7e72c9f14a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Recv_tensor : tensor([[[[ 0,  1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8,  9],\n",
       "          [10, 11, 12, 13, 14]],\n",
       "\n",
       "         [[60, 61, 62, 63, 64],\n",
       "          [65, 66, 67, 68, 69],\n",
       "          [70, 71, 72, 73, 74]]],\n",
       "\n",
       "\n",
       "        [[[30, 31, 32, 33, 34],\n",
       "          [35, 36, 37, 38, 39],\n",
       "          [40, 41, 42, 43, 44]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Permuted data tensor shape: torch.Size([2, 2, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R2: Permuted data tensor: tensor([[[[ 0,  1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8,  9],\n",
       "          [10, 11, 12, 13, 14]],\n",
       "\n",
       "         [[30, 31, 32, 33, 34],\n",
       "          [35, 36, 37, 38, 39],\n",
       "          [40, 41, 42, 43, 44]]],\n",
       "\n",
       "\n",
       "        [[[60, 61, 62, 63, 64],\n",
       "          [65, 66, 67, 68, 69],\n",
       "          [70, 71, 72, 73, 74]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] 17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Local tensor shape: torch.Size([3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Expected local shape: torch.Size([3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Changed tensor axis: 0, minor: False\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: New distribution: D_[2,2]⊥{1,∅}(3,∅), new shape: torch.Size([9, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Processor multi index: torch.Size([1, 1])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: N procs: 2\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Rank of largest tensor in the subcommunicator: [0, 1] 1\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: N elements: 30\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Max local shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Send buffer: (<mpi4py.MPI.buffer object at 0x78f5f24091b0>, 15, <mpi4py.MPI.Datatype object at 0x78f6e5e18a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Recv buffer: (<mpi4py.MPI.buffer object at 0x78f5f2409220>, 30, <mpi4py.MPI.Datatype object at 0x78f6e5e18a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Recv_tensor : tensor([[[[15, 16, 17, 18, 19],\n",
       "          [20, 21, 22, 23, 24],\n",
       "          [25, 26, 27, 28, 29]],\n",
       "\n",
       "         [[75, 76, 77, 78, 79],\n",
       "          [80, 81, 82, 83, 84],\n",
       "          [85, 86, 87, 88, 89]]],\n",
       "\n",
       "\n",
       "        [[[45, 46, 47, 48, 49],\n",
       "          [50, 51, 52, 53, 54],\n",
       "          [55, 56, 57, 58, 59]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Permuted data tensor shape: torch.Size([2, 2, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R3: Permuted data tensor: tensor([[[[15, 16, 17, 18, 19],\n",
       "          [20, 21, 22, 23, 24],\n",
       "          [25, 26, 27, 28, 29]],\n",
       "\n",
       "         [[45, 46, 47, 48, 49],\n",
       "          [50, 51, 52, 53, 54],\n",
       "          [55, 56, 57, 58, 59]]],\n",
       "\n",
       "\n",
       "        [[[75, 76, 77, 78, 79],\n",
       "          [80, 81, 82, 83, 84],\n",
       "          [85, 86, 87, 88, 89]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] 17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Local tensor shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Expected local shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Changed tensor axis: 0, minor: False\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: New distribution: D_[2,2]⊥{1,∅}(3,∅), new shape: torch.Size([9, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Processor multi index: torch.Size([0, 0])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: N procs: 2\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Rank of largest tensor in the subcommunicator: [0, 0] 0\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: N elements: 30\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Max local shape: torch.Size([6, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Send buffer: (<mpi4py.MPI.buffer object at 0x77c83d9ed140>, 30, <mpi4py.MPI.Datatype object at 0x77c931410a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Recv buffer: (<mpi4py.MPI.buffer object at 0x77c83d9ed220>, 30, <mpi4py.MPI.Datatype object at 0x77c931410a20>)\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Recv_tensor : tensor([[[[ 0,  1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8,  9],\n",
       "          [10, 11, 12, 13, 14]],\n",
       "\n",
       "         [[60, 61, 62, 63, 64],\n",
       "          [65, 66, 67, 68, 69],\n",
       "          [70, 71, 72, 73, 74]]],\n",
       "\n",
       "\n",
       "        [[[30, 31, 32, 33, 34],\n",
       "          [35, 36, 37, 38, 39],\n",
       "          [40, 41, 42, 43, 44]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Permuted data tensor shape: torch.Size([2, 2, 3, 5])\n",
       "17-04-2025 03:32:01 : INFO : multi_axis : apply_allgather -- R0: Permuted data tensor: tensor([[[[ 0,  1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8,  9],\n",
       "          [10, 11, 12, 13, 14]],\n",
       "\n",
       "         [[30, 31, 32, 33, 34],\n",
       "          [35, 36, 37, 38, 39],\n",
       "          [40, 41, 42, 43, 44]]],\n",
       "\n",
       "\n",
       "        [[[60, 61, 62, 63, 64],\n",
       "          [65, 66, 67, 68, 69],\n",
       "          [70, 71, 72, 73, 74]],\n",
       "\n",
       "         [[ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:2] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "dist.apply_allgather(x.shape, x_local, comm, mesh_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorcraft-dev311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
