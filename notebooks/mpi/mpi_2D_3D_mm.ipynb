{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D and 3D Matrix Multiplication\n",
    "## Setup\n",
    "### Installation\n",
    "```pip install ipyparallel```\n",
    "\n",
    "or \n",
    "\n",
    "```pip install -e .[notebook]```\n",
    "\n",
    "### Start cluster\n",
    "\n",
    "```ipcluster start -n 4 --engines=MPI --profile mpi```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile='mpi')\n",
    "rc.wait_for_engines(4)\n",
    "len(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import torch\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print(f'Hello from rank {rank}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "cart_comm = comm.Create_cart(dims=[2, 2], periods=[True, True], reorder=True)\n",
    "print(f'Hello from rank {rank}! My coordinates are {cart_comm.Get_coords(rank)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(f\"Topo: {cart_comm.Get_topo()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "row_comm = cart_comm.Sub([0,1])\n",
    "row_global_ranks = row_comm.allgather(rank)\n",
    "print(f'Hello from rank {rank}! In my row, the ranks are {row_global_ranks}')\n",
    "\n",
    "col_comm = cart_comm.Sub([1,0])\n",
    "col_global_ranks = col_comm.allgather(rank)\n",
    "print(f'Hello from rank {rank}! In my col, the ranks are {col_global_ranks}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def as_buffer(x: torch.Tensor):\n",
    "    return MPI.buffer.fromaddress(x.untyped_storage().data_ptr(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Vector\n",
    "\n",
    "### y:= Ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A_g = torch.arange(64).reshape(8, 8)\n",
    "A_l = A_g[col_comm.Get_rank()::2, row_comm.Get_rank()::2].contiguous()\n",
    "print(\"A_local:\", A_l)\n",
    "\n",
    "x_g = torch.arange(8).reshape(8, 1)\n",
    "x_l = x_g[comm.Get_rank()::comm.Get_size(), :].contiguous()\n",
    "print(\"x_local:\", x_l)\n",
    "\n",
    "\n",
    "# x_col = torch.zeros(4,1, dtype=torch.long)\n",
    "# col_comm.Allgather((as_buffer(x_l), 2, MPI.LONG), (as_buffer(x_col), 2, MPI.LONG))\n",
    "# x_col = x_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "# print(\"x_gather_col:\", x_col)\n",
    "\n",
    "x_col = torch.zeros(4,1, dtype=torch.long)\n",
    "data_type = MPI.LONG.Create_vector(2, 1, 2).Create_resized(MPI.LONG.Get_extent()[0], MPI.LONG.Get_extent()[1]).Commit()\n",
    "col_comm.Allgather((as_buffer(x_l), 2, MPI.LONG), (as_buffer(x_col), 1, data_type))\n",
    "print(\"x_gather_col:\", x_col)\n",
    "\n",
    "y_l = A_l @ x_col\n",
    "print(\"y_local:\", y_l)\n",
    "\n",
    "y_l = y_l.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "y_scatter = torch.zeros(2, 1, dtype=torch.long)\n",
    "row_comm.Reduce_scatter((as_buffer(y_l), 4, MPI.LONG), (as_buffer(y_scatter), 2, MPI.LONG), [2,2], MPI.SUM)\n",
    "print(\"y_scatter:\", y_scatter)\n",
    "\n",
    "y_end = torch.zeros(2,2,2, dtype=torch.long)\n",
    "comm.Allgather((as_buffer(y_scatter), 2, MPI.LONG), (as_buffer(y_end), 2, MPI.LONG))\n",
    "y_end = y_end.permute(2,1,0).reshape(8,1).contiguous()\n",
    "print(\"y_end:\", y_end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"Expected:\", A_g @ x_g)\n",
    "print(\"Actual:\", y_end)\n",
    "torch.allclose(y_end, A_g @ x_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x = A.T * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A_g = torch.arange(64).reshape(8, 8)\n",
    "A_l = A_g[col_comm.Get_rank()::2, row_comm.Get_rank()::2].contiguous()\n",
    "print(\"A_local:\", A_l)\n",
    "\n",
    "y_g = torch.arange(8).reshape(8, 1)\n",
    "i = col_comm.Get_rank() + col_comm.Get_size() * row_comm.Get_rank()\n",
    "y_l = y_g[i::comm.Get_size(), :].contiguous()\n",
    "print(\"y_local:\", y_l)\n",
    "\n",
    "y_col = torch.zeros(4,1, dtype=torch.long)\n",
    "row_comm.Allgather((as_buffer(y_l), 2, MPI.LONG), (as_buffer(y_col), 2, MPI.LONG))\n",
    "y_col = y_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "print(\"y_gather_col:\", y_col)\n",
    "\n",
    "x_l = A_l.T @ y_col\n",
    "print(\"x_local:\", x_l)\n",
    "\n",
    "x_l = x_l.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "x_scatter = torch.zeros(2, 1, dtype=torch.long)\n",
    "col_comm.Reduce_scatter((as_buffer(x_l), 4, MPI.LONG), (as_buffer(x_scatter), 2, MPI.LONG), [2,2], MPI.SUM)\n",
    "print(\"x_scatter:\", x_scatter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"expected:\", A_g.T @ y_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A := y * x.T + A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "A_g = torch.arange(64).reshape(8, 8)\n",
    "A_l = A_g[col_comm.Get_rank()::2, row_comm.Get_rank()::2].contiguous()\n",
    "print(\"A_local:\", A_l)\n",
    "\n",
    "x_g = torch.arange(8).reshape(8, 1)\n",
    "x_l = x_g[comm.Get_rank()::comm.Get_size(), :].contiguous()\n",
    "print(\"x_local:\", x_l)\n",
    "\n",
    "y_g = torch.arange(8).reshape(8, 1)\n",
    "i = col_comm.Get_rank() + col_comm.Get_size() * row_comm.Get_rank()\n",
    "y_l = y_g[i::comm.Get_size(), :].contiguous()\n",
    "print(\"y_local:\", y_l)\n",
    "\n",
    "x_col = torch.zeros(4,1, dtype=torch.long)\n",
    "col_comm.Allgather((as_buffer(x_l), 2, MPI.LONG), (as_buffer(x_col), 2, MPI.LONG))\n",
    "x_col = x_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "print(\"x_gather_col:\", x_col)\n",
    "\n",
    "y_col = torch.zeros(4,1, dtype=torch.long)\n",
    "row_comm.Allgather((as_buffer(y_l), 2, MPI.LONG), (as_buffer(y_col), 2, MPI.LONG))\n",
    "y_col = y_col.reshape(2,2).T.reshape(4,1).contiguous()\n",
    "print(\"y_gather_col:\", y_col)\n",
    "\n",
    "Z_l = y_col @ x_col.T + A_l\n",
    "print(\"Z_local:\", Z_l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(f\"Expected: {y_g @ x_g.T + A_g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create_darray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if comm.Get_rank() == 0:\n",
    "    A = torch.arange(64).reshape(8, 8)\n",
    "    print(A)\n",
    "    print(A.dtype)\n",
    "\n",
    "    darray_type = MPI.LONG.Create_darray(\n",
    "        4,                    # Size\n",
    "        1,                      # Rank\n",
    "        # 2,                      # number of array dimensions (as well as process grid dimensions)\n",
    "        [64,1],                 # size of the global array\n",
    "        [MPI.DISTRIBUTE_CYCLIC, MPI.DISTRIBUTE_NONE], # distribution type\n",
    "        [1, 1], # distribution argument\n",
    "        [4, 1],                 # size of the process grid\n",
    "        MPI.ORDER_C,            # array storage order\n",
    "    ).Commit()\n",
    "\n",
    "    # comm.Send(buf=[as_buffer(A), 8, MPI.LONG], dest=1)\n",
    "    comm.Send([as_buffer(A), 1, darray_type], dest=1, tag=55)\n",
    "\n",
    "    darray_type.Free() \n",
    "\n",
    "elif comm.Get_rank() == 1:\n",
    "    A = torch.zeros(4, 4, dtype=torch.int64)\n",
    "\n",
    "    # comm.Recv(buf=[as_buffer(A), 8, MPI.LONG], source=0)\n",
    "    comm.Recv([as_buffer(A), 16, MPI.LONG], source=0, tag=55)\n",
    "    print(A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Item Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import tensorcraft as tc\n",
    "comm = MPI.COMM_WORLD\n",
    "## Let's find the maximum element count on my mpi implementation\n",
    "options = [torch.iinfo(torch.int32).max]\n",
    "\n",
    "for possible_max in options:\n",
    "    print(f\"Trying {possible_max}\")\n",
    "    if comm.Get_rank() == 0:\n",
    "        A = torch.ones(possible_max, dtype=torch.bool)\n",
    "        print(A.dtype)\n",
    "        print(A[:10])\n",
    "\n",
    "        print(f\"Sending {possible_max} elements, {possible_max / 10**9} Gb\")\n",
    "\n",
    "    else:\n",
    "        A = torch.zeros(possible_max, dtype=torch.bool)\n",
    "\n",
    "    comm.Bcast(buf=[tc.mpi4torch.as_buffer(A), possible_max, MPI.BOOL], root=0)\n",
    "\n",
    "    if comm.Get_rank() == 0:\n",
    "        print(\"Sent!\")\n",
    "    else:\n",
    "        print(\"Received!\")\n",
    "        print(A[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interweave allgather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile='mpi')\n",
    "rc.wait_for_engines(4)\n",
    "len(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%px:   0%|          | 0/4 [00:36<?, ?tasks/s]"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import torch\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print(f'Hello from rank {rank}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%px:   0%|          | 0/4 [00:02<?, ?tasks/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] 22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Processor multi index: torch.Size([0, 0])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Missing elements: [0, 1, 0]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: N blocks per axis: [1, 10, 1]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Padded tensor shape: torch.Size([2, 11, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Permute tuple: (0, 2, 4, 1, 3, 5)\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Reshape tuple: [1, 2, 11, 1, 1, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Tile Slices: [slice(None, None, None), slice(None, None, None), slice(tensor(0), None, 4), slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Local tensor shape: torch.Size([1, 2, 3, 1, 1, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Target local tensor shape: [2, 3, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Linear processor index: 0, Residue: 1, Block size: 1, axis: 1, N procs: 4, N full blocks: 10\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Shaved slices: [slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R0: Final local tensor shape: torch.Size([2, 3, 2])\n",
       "tensor([[[ 0,  1],\n",
       "         [ 8,  9],\n",
       "         [16, 17]],\n",
       "\n",
       "        [[20, 21],\n",
       "         [28, 29],\n",
       "         [36, 37]]])\n",
       "torch.Size([2, 3, 2])\n",
       "torch.int64\n",
       "False\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] 22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Processor multi index: torch.Size([0, 1])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Missing elements: [0, 1, 0]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: N blocks per axis: [1, 10, 1]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Padded tensor shape: torch.Size([2, 11, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Permute tuple: (0, 2, 4, 1, 3, 5)\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Reshape tuple: [1, 2, 11, 1, 1, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Tile Slices: [slice(None, None, None), slice(None, None, None), slice(tensor(1), None, 4), slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Local tensor shape: torch.Size([1, 2, 3, 1, 1, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Target local tensor shape: [2, 3, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Linear processor index: 1, Residue: 1, Block size: 1, axis: 1, N procs: 4, N full blocks: 10\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Shaved slices: [slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R1: Final local tensor shape: torch.Size([2, 3, 2])\n",
       "tensor([[[ 2,  3],\n",
       "         [10, 11],\n",
       "         [18, 19]],\n",
       "\n",
       "        [[22, 23],\n",
       "         [30, 31],\n",
       "         [38, 39]]])\n",
       "torch.Size([2, 3, 2])\n",
       "torch.int64\n",
       "False\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%px:   0%|          | 0/4 [00:02<?, ?tasks/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] 22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Processor multi index: torch.Size([1, 1])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Missing elements: [0, 1, 0]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: N blocks per axis: [1, 10, 1]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Padded tensor shape: torch.Size([2, 11, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Permute tuple: (0, 2, 4, 1, 3, 5)\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Reshape tuple: [1, 2, 11, 1, 1, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Tile Slices: [slice(None, None, None), slice(None, None, None), slice(tensor(3), None, 4), slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Local tensor shape: torch.Size([1, 2, 2, 1, 1, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Target local tensor shape: [2, 2, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Linear processor index: 3, Residue: 1, Block size: 1, axis: 1, N procs: 4, N full blocks: 10\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Shaved slices: [slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R3: Final local tensor shape: torch.Size([2, 2, 2])\n",
       "tensor([[[ 6,  7],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[26, 27],\n",
       "         [34, 35]]])\n",
       "torch.Size([2, 2, 2])\n",
       "torch.int64\n",
       "False\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] 22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Processor multi index: torch.Size([1, 0])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Missing elements: [0, 1, 0]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: N blocks per axis: [1, 10, 1]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Padded tensor shape: torch.Size([2, 11, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Permute tuple: (0, 2, 4, 1, 3, 5)\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Reshape tuple: [1, 2, 11, 1, 1, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Tile Slices: [slice(None, None, None), slice(None, None, None), slice(tensor(2), None, 4), slice(None, None, None), slice(None, None, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Local tensor shape: torch.Size([1, 2, 3, 1, 1, 2])\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Target local tensor shape: [2, 3, 2]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Linear processor index: 2, Residue: 1, Block size: 1, axis: 1, N procs: 4, N full blocks: 10\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Shaved slices: [slice(None, None, None), slice(0, -1, None), slice(None, None, None)]\n",
       "22-04-2025 11:42:18 : INFO : multi_axis : apply -- R2: Final local tensor shape: torch.Size([2, 2, 2])\n",
       "tensor([[[ 4,  5],\n",
       "         [12, 13]],\n",
       "\n",
       "        [[24, 25],\n",
       "         [32, 33]]])\n",
       "torch.Size([2, 2, 2])\n",
       "torch.int64\n",
       "False\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%px: 100%|██████████| 4/4 [00:02<00:00,  1.82tasks/s]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import logging\n",
    "import tensorcraft as tc\n",
    "\n",
    "log = logging.getLogger('tensorcraft')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "x = torch.arange(40).reshape(2, 10, 2)\n",
    "mesh = torch.Size([2,2])\n",
    "dist = tc.dist.MultiAxisDist(mesh, (None, (0,1), None), 1)\n",
    "\n",
    "x_local = dist.apply(x, rank)\n",
    "print(x_local)\n",
    "print(x_local.shape)\n",
    "print(x_local.dtype)\n",
    "print(x_local.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] 22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Local tensor shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Expected local shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Changed tensor axis: 1, minor: False\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: New distribution: D_[2,2]⊥{∅,1,∅}(∅,1,∅), new shape: torch.Size([2, 5, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Processor multi index: torch.Size([0, 0])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: N procs: 2\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Rank of largest tensor in the subcommunicator: [0, 0] 0\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: N elements: 12\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Max local shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Padding: 0\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Send buffer: (<mpi4py.MPI.buffer object at 0x77dc3280cf80>, 1, <mpi4py.MPI.Datatype object at 0x77dc35733570>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Recv buffer: (<mpi4py.MPI.buffer object at 0x77dc3280cf10>, 12, <mpi4py.MPI.Datatype object at 0x77dc35733570>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R0: Recv_tensor : tensor([[[[[ 0,  1]],\n",
       "\n",
       "          [[ 8,  9]],\n",
       "\n",
       "          [[16, 17]]],\n",
       "\n",
       "\n",
       "         [[[20, 21]],\n",
       "\n",
       "          [[28, 29]],\n",
       "\n",
       "          [[36, 37]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4,  5]],\n",
       "\n",
       "          [[12, 13]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[24, 25]],\n",
       "\n",
       "          [[32, 33]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n",
       "tensor([[[[[ 0,  1]],\n",
       "\n",
       "          [[ 8,  9]],\n",
       "\n",
       "          [[16, 17]]],\n",
       "\n",
       "\n",
       "         [[[20, 21]],\n",
       "\n",
       "          [[28, 29]],\n",
       "\n",
       "          [[36, 37]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4,  5]],\n",
       "\n",
       "          [[12, 13]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[24, 25]],\n",
       "\n",
       "          [[32, 33]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] 22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Local tensor shape: torch.Size([2, 2, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Expected local shape: torch.Size([2, 2, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Changed tensor axis: 1, minor: False\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: New distribution: D_[2,2]⊥{∅,1,∅}(∅,1,∅), new shape: torch.Size([2, 5, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Processor multi index: torch.Size([1, 0])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: N procs: 2\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Rank of largest tensor in the subcommunicator: [0, 0] 0\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: N elements: 12\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Max local shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Padding: 1\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Padding tuple: [0, 0, 1, 0, 0, 0]\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Padded local tensor shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Padded local tensor: tensor([[[ 4,  5],\n",
       "         [12, 13],\n",
       "         [ 0,  0]],\n",
       "\n",
       "        [[24, 25],\n",
       "         [32, 33],\n",
       "         [ 0,  0]]])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Send buffer: (<mpi4py.MPI.buffer object at 0x73a28f9e5060>, 12, <mpi4py.MPI.Datatype object at 0x73a3833d0a20>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Recv buffer: (<mpi4py.MPI.buffer object at 0x73a28f9e4ea0>, 12, <mpi4py.MPI.Datatype object at 0x73a3833d0a20>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R2: Recv_tensor : tensor([[[[[ 0,  1]],\n",
       "\n",
       "          [[ 8,  9]],\n",
       "\n",
       "          [[16, 17]]],\n",
       "\n",
       "\n",
       "         [[[20, 21]],\n",
       "\n",
       "          [[28, 29]],\n",
       "\n",
       "          [[36, 37]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4,  5]],\n",
       "\n",
       "          [[12, 13]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[24, 25]],\n",
       "\n",
       "          [[32, 33]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n",
       "tensor([[[[[ 0,  1]],\n",
       "\n",
       "          [[ 8,  9]],\n",
       "\n",
       "          [[16, 17]]],\n",
       "\n",
       "\n",
       "         [[[20, 21]],\n",
       "\n",
       "          [[28, 29]],\n",
       "\n",
       "          [[36, 37]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4,  5]],\n",
       "\n",
       "          [[12, 13]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[24, 25]],\n",
       "\n",
       "          [[32, 33]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] 22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Local tensor shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Expected local shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Changed tensor axis: 1, minor: False\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: New distribution: D_[2,2]⊥{∅,1,∅}(∅,1,∅), new shape: torch.Size([2, 5, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Processor multi index: torch.Size([0, 1])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: N procs: 2\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Rank of largest tensor in the subcommunicator: [0, 1] 1\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: N elements: 12\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Max local shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Padding: 0\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Send buffer: (<mpi4py.MPI.buffer object at 0x73b9843f0ea0>, 1, <mpi4py.MPI.Datatype object at 0x73ba7428ced0>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Recv buffer: (<mpi4py.MPI.buffer object at 0x73b9843f0ff0>, 12, <mpi4py.MPI.Datatype object at 0x73ba7428ced0>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R1: Recv_tensor : tensor([[[[[ 2,  3]],\n",
       "\n",
       "          [[10, 11]],\n",
       "\n",
       "          [[18, 19]]],\n",
       "\n",
       "\n",
       "         [[[22, 23]],\n",
       "\n",
       "          [[30, 31]],\n",
       "\n",
       "          [[38, 39]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 6,  7]],\n",
       "\n",
       "          [[14, 15]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[26, 27]],\n",
       "\n",
       "          [[34, 35]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n",
       "tensor([[[[[ 2,  3]],\n",
       "\n",
       "          [[10, 11]],\n",
       "\n",
       "          [[18, 19]]],\n",
       "\n",
       "\n",
       "         [[[22, 23]],\n",
       "\n",
       "          [[30, 31]],\n",
       "\n",
       "          [[38, 39]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 6,  7]],\n",
       "\n",
       "          [[14, 15]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[26, 27]],\n",
       "\n",
       "          [[34, 35]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] 22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Local tensor shape: torch.Size([2, 2, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Expected local shape: torch.Size([2, 2, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Changed tensor axis: 1, minor: False\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: New distribution: D_[2,2]⊥{∅,1,∅}(∅,1,∅), new shape: torch.Size([2, 5, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Processor multi index: torch.Size([1, 1])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: N procs: 2\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Rank of largest tensor in the subcommunicator: [0, 1] 1\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: N elements: 12\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Max local shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Padding: 1\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Padding tuple: [0, 0, 1, 0, 0, 0]\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Padded local tensor shape: torch.Size([2, 3, 2])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Padded local tensor: tensor([[[ 6,  7],\n",
       "         [14, 15],\n",
       "         [ 0,  0]],\n",
       "\n",
       "        [[26, 27],\n",
       "         [34, 35],\n",
       "         [ 0,  0]]])\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Send buffer: (<mpi4py.MPI.buffer object at 0x7ae7a8bd9060>, 12, <mpi4py.MPI.Datatype object at 0x7ae89c548a20>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Recv buffer: (<mpi4py.MPI.buffer object at 0x7ae7a8bd8f10>, 12, <mpi4py.MPI.Datatype object at 0x7ae89c548a20>)\n",
       "22-04-2025 11:42:19 : INFO : multi_axis : apply_allgather -- R3: Recv_tensor : tensor([[[[[ 2,  3]],\n",
       "\n",
       "          [[10, 11]],\n",
       "\n",
       "          [[18, 19]]],\n",
       "\n",
       "\n",
       "         [[[22, 23]],\n",
       "\n",
       "          [[30, 31]],\n",
       "\n",
       "          [[38, 39]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 6,  7]],\n",
       "\n",
       "          [[14, 15]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[26, 27]],\n",
       "\n",
       "          [[34, 35]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n",
       "tensor([[[[[ 2,  3]],\n",
       "\n",
       "          [[10, 11]],\n",
       "\n",
       "          [[18, 19]]],\n",
       "\n",
       "\n",
       "         [[[22, 23]],\n",
       "\n",
       "          [[30, 31]],\n",
       "\n",
       "          [[38, 39]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 6,  7]],\n",
       "\n",
       "          [[14, 15]],\n",
       "\n",
       "          [[ 0,  0]]],\n",
       "\n",
       "\n",
       "         [[[26, 27]],\n",
       "\n",
       "          [[34, 35]],\n",
       "\n",
       "          [[ 0,  0]]]]])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:2] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] /home/juanpedroghm/code/tensorcraft/tensorcraft/util/axis_utils.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
       "  index_tensor = torch.tensor(index)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "post_gather, new_dist = dist.apply_allgather(x.shape, x_local, comm, mesh_dim=0)\n",
    "print(post_gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] torch.Size([2, 2, 3, 1, 2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] torch.Size([2, 2, 3, 1, 2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:7]: \u001b[0m\n",
       "tensor([[[[[ 0,  1]],\n",
       "\n",
       "          [[ 4,  5]]],\n",
       "\n",
       "\n",
       "         [[[ 8,  9]],\n",
       "\n",
       "          [[12, 13]]],\n",
       "\n",
       "\n",
       "         [[[16, 17]],\n",
       "\n",
       "          [[ 0,  0]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[20, 21]],\n",
       "\n",
       "          [[24, 25]]],\n",
       "\n",
       "\n",
       "         [[[28, 29]],\n",
       "\n",
       "          [[32, 33]]],\n",
       "\n",
       "\n",
       "         [[[36, 37]],\n",
       "\n",
       "          [[ 0,  0]]]]])"
      ]
     },
     "metadata": {
      "after": null,
      "completed": null,
      "data": {},
      "engine_id": 0,
      "engine_uuid": "e1b31fe9-deced48fbd3bb0b9424e1aed",
      "error": null,
      "execute_input": "print(post_gather.shape)\npost_gather.permute(1,2,0,3,4)\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([[[[[ 0,  1]],\n\n          [[ 4,  5]]],\n\n\n         [[[ 8,  9]],\n\n          [[12, 13]]],\n\n\n         [[[16, 17]],\n\n          [[ 0,  0]]]],\n\n\n\n        [[[[20, 21]],\n\n          [[24, 25]]],\n\n\n         [[[28, 29]],\n\n          [[32, 33]]],\n\n\n         [[[36, 37]],\n\n          [[ 0,  0]]]]])"
       },
       "execution_count": 7,
       "metadata": {}
      },
      "follow": null,
      "msg_id": null,
      "outputs": [],
      "received": null,
      "started": null,
      "status": null,
      "stderr": "",
      "stdout": "torch.Size([2, 2, 3, 1, 2])\n",
      "submitted": "2025-04-22T09:44:37.464514Z"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:7]: \u001b[0m\n",
       "tensor([[[[[ 2,  3]],\n",
       "\n",
       "          [[ 6,  7]]],\n",
       "\n",
       "\n",
       "         [[[10, 11]],\n",
       "\n",
       "          [[14, 15]]],\n",
       "\n",
       "\n",
       "         [[[18, 19]],\n",
       "\n",
       "          [[ 0,  0]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[22, 23]],\n",
       "\n",
       "          [[26, 27]]],\n",
       "\n",
       "\n",
       "         [[[30, 31]],\n",
       "\n",
       "          [[34, 35]]],\n",
       "\n",
       "\n",
       "         [[[38, 39]],\n",
       "\n",
       "          [[ 0,  0]]]]])"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2025-04-22T09:44:37.472155Z",
      "data": {},
      "engine_id": 1,
      "engine_uuid": "cb79769c-f84f4db10b61b7d4f5a2f882",
      "error": null,
      "execute_input": "print(post_gather.shape)\npost_gather.permute(1,2,0,3,4)\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([[[[[ 2,  3]],\n\n          [[ 6,  7]]],\n\n\n         [[[10, 11]],\n\n          [[14, 15]]],\n\n\n         [[[18, 19]],\n\n          [[ 0,  0]]]],\n\n\n\n        [[[[22, 23]],\n\n          [[26, 27]]],\n\n\n         [[[30, 31]],\n\n          [[34, 35]]],\n\n\n         [[[38, 39]],\n\n          [[ 0,  0]]]]])"
       },
       "execution_count": 7,
       "metadata": {}
      },
      "follow": [],
      "is_broadcast": false,
      "is_coalescing": false,
      "msg_id": "c4053001-8ea654cec7eadce53f5df933_70100_26",
      "outputs": [],
      "received": "2025-04-22T09:44:37.478996Z",
      "started": "2025-04-22T09:44:37.467572Z",
      "status": "ok",
      "stderr": "",
      "stdout": "torch.Size([2, 2, 3, 1, 2])\n",
      "submitted": "2025-04-22T09:44:37.464569Z"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] torch.Size([2, 2, 3, 1, 2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:7]: \u001b[0m\n",
       "tensor([[[[[ 0,  1]],\n",
       "\n",
       "          [[ 4,  5]]],\n",
       "\n",
       "\n",
       "         [[[ 8,  9]],\n",
       "\n",
       "          [[12, 13]]],\n",
       "\n",
       "\n",
       "         [[[16, 17]],\n",
       "\n",
       "          [[ 0,  0]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[20, 21]],\n",
       "\n",
       "          [[24, 25]]],\n",
       "\n",
       "\n",
       "         [[[28, 29]],\n",
       "\n",
       "          [[32, 33]]],\n",
       "\n",
       "\n",
       "         [[[36, 37]],\n",
       "\n",
       "          [[ 0,  0]]]]])"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2025-04-22T09:44:37.477392Z",
      "data": {},
      "engine_id": 2,
      "engine_uuid": "24064844-875ef5695a18e071a3429994",
      "error": null,
      "execute_input": "print(post_gather.shape)\npost_gather.permute(1,2,0,3,4)\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([[[[[ 0,  1]],\n\n          [[ 4,  5]]],\n\n\n         [[[ 8,  9]],\n\n          [[12, 13]]],\n\n\n         [[[16, 17]],\n\n          [[ 0,  0]]]],\n\n\n\n        [[[[20, 21]],\n\n          [[24, 25]]],\n\n\n         [[[28, 29]],\n\n          [[32, 33]]],\n\n\n         [[[36, 37]],\n\n          [[ 0,  0]]]]])"
       },
       "execution_count": 7,
       "metadata": {}
      },
      "follow": [],
      "is_broadcast": false,
      "is_coalescing": false,
      "msg_id": "c4053001-8ea654cec7eadce53f5df933_70100_27",
      "outputs": [],
      "received": "2025-04-22T09:44:37.483469Z",
      "started": "2025-04-22T09:44:37.468978Z",
      "status": "ok",
      "stderr": "",
      "stdout": "torch.Size([2, 2, 3, 1, 2])\n",
      "submitted": "2025-04-22T09:44:37.465199Z"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] torch.Size([2, 2, 3, 1, 2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:7]: \u001b[0m\n",
       "tensor([[[[[ 2,  3]],\n",
       "\n",
       "          [[ 6,  7]]],\n",
       "\n",
       "\n",
       "         [[[10, 11]],\n",
       "\n",
       "          [[14, 15]]],\n",
       "\n",
       "\n",
       "         [[[18, 19]],\n",
       "\n",
       "          [[ 0,  0]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[22, 23]],\n",
       "\n",
       "          [[26, 27]]],\n",
       "\n",
       "\n",
       "         [[[30, 31]],\n",
       "\n",
       "          [[34, 35]]],\n",
       "\n",
       "\n",
       "         [[[38, 39]],\n",
       "\n",
       "          [[ 0,  0]]]]])"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2025-04-22T09:44:37.481436Z",
      "data": {},
      "engine_id": 3,
      "engine_uuid": "25e60af0-264809eb8a45f2841ba3782e",
      "error": null,
      "execute_input": "print(post_gather.shape)\npost_gather.permute(1,2,0,3,4)\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([[[[[ 2,  3]],\n\n          [[ 6,  7]]],\n\n\n         [[[10, 11]],\n\n          [[14, 15]]],\n\n\n         [[[18, 19]],\n\n          [[ 0,  0]]]],\n\n\n\n        [[[[22, 23]],\n\n          [[26, 27]]],\n\n\n         [[[30, 31]],\n\n          [[34, 35]]],\n\n\n         [[[38, 39]],\n\n          [[ 0,  0]]]]])"
       },
       "execution_count": 7,
       "metadata": {}
      },
      "follow": [],
      "is_broadcast": false,
      "is_coalescing": false,
      "msg_id": "c4053001-8ea654cec7eadce53f5df933_70100_28",
      "outputs": [],
      "received": "2025-04-22T09:44:37.484249Z",
      "started": "2025-04-22T09:44:37.468258Z",
      "status": "ok",
      "stderr": "",
      "stdout": "torch.Size([2, 2, 3, 1, 2])\n",
      "submitted": "2025-04-22T09:44:37.465513Z"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "print(post_gather.shape)\n",
    "post_gather.permute(1,2,0,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorcraft-dev311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
